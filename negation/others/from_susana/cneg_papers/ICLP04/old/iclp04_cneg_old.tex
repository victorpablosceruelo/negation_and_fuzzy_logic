%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Implementation of Constructive Negation for Prolog 
% (con el ingles corregido)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass{llncs}

%% \newenvironment{mytabbing}
%%    {\vspace{0.3em}\begin{small}\begin{tabbing}}
%%    {\end{tabbing}\end{small}\vspace{0.3em}}

%%\newenvironment{mytabbing}
%%   {\begin{tabbing}}
%%   {\end{tabbing}}

\usepackage{pst-node}

\usepackage{amsmath} %% for functions defined using different parts
\usepackage{amssymb}
\usepackage{latexsym}
\usepackage{times}
\usepackage{mathptmx}
\usepackage{theorem}


\newcommand{\naf}{{\em naf}}\newcommand{\viejo}[1]{}
\newcommand{\ciao}{Ciao}


\newcommand{\tab}{\hspace{2em}}
\newcommand{\ra}{$\rightarrow~$}
\newcommand{\Ra}{\Rightarrow~}
\newcommand{\HINT}{{\cal H\!-\!INT}}
%\newcommand{\cts}{\mid}
\newcommand{\cts}{~[\!]~}
\newcommand{\N}{I\!\!N}
\newcommand{\entails}{\models}
\newcommand{\vecy}{\overline{y}}

\newcommand{\ToDo}[1]{
  \begin{center}
      \begin{minipage}{0.75\textwidth}
        \hrule
        \textbf{To do:}\\
        {\em #1}
        \hrule
      \end{minipage}\\
  \end{center}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Implementation Results in \\
       Classical Constructive Negation}

%\author{Susana Mu\~{n}oz~~~~~~~~~~~~~~~~~ Juan Jos\'{e} Moreno-Navarro \\
%         \email{susana@fi.upm.es}~~~~~~~~~~ \email{jjmoreno@fi.upm.es}}
\author{Susana Mu\~{n}oz Hern\'{a}ndez~~~~ Juan Jos\'{e} Moreno-Navarro \\
 \email{~~susana@fi.upm.es}~~~~ \email{jjmoreno@fi.upm.es ~}}

\institute{ 
LSIIS, Facultad de Inform\'{a}tica \\
Universidad Polit\'{e}cnica de  Madrid \\ 
Campus de Montegancedo s/n Boadilla del Monte\\
28660 Madrid, Spain \footnote{This work was partly supported by the
Spanish MCYT project TIC2000-1632.} \\
 }

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%
%\vspace{-12pt}

%



\begin{abstract}
  Logic Programming has been advocated as a language for system
  specification, especially for those involving logical behaviours,
  rules and knowledge. However, modeling problems involving negation,
  which is quite natural in many cases, is somewhat limited if Prolog
  is used as the specification / implementation language. These
  restrictions are not related to theory viewpoint, where users can
  find many different models with their respective semantics; they
  concern practical implementation issues.  The negation capabilities
  supported by current Prolog systems are rather constrained, and
  there is no a correct and complete implementation.  In this paper,
  we refine and propose some extensions to the classical method of
  constructive negation, providing the complete theoretical
  algorithm. Furthermore, we also discuss implementation issues
  providing a preliminary implementation and an optimized
  implementation to negate predicates with a finite number of
  solutions.
\end{abstract}

\paragraph{\bf Keywords}
Constructive Negation, Negation in Logic Programming, Constraint Logic
Programming, Implementations of Logic Programming, Optimization.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%% INTRODUCTION %%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
\label{introduction}
From its very beginning Logic Programming has been advocated to be
both a programming language and a specification language. It is
natural to use Logic Programming for specifying/programming systems
involving logical behaviours, rules and knowledge. However, this idea
has a severe limitation: the use of negation. Negation is probably the
most significant aspect of logic that was not included from the
outset. This is due to the fact that dealing with negation involves
significant additional complexity. Nevertheless, the use of negation
is very natural and plays an important role in many cases, for
instance, constraints management in databases, program composition,
manipulation and transformation, default reasoning, natural language
processing, etc.

% Work done in negation
Although this restriction cannot be perceived from the theoretical point of
view (because there are many alternative ways to understand and
incorporate negation into Logic Programming), the problems really start
at the semantic level, where the different proposals (negation as
failure -\naf-, stable models, well-founded semantics, explicit
negation, etc.)  differ not only as to expressiveness but also as to
semantics.  However, the negation techniques supported by current
Prolog compilers are rather limited, restricted to negation as failure
under Fitting/Kunen semantics \cite{Kunen} (sound only under some
circumstances usually not checked by compilers) which is a built-in or
library in most Prolog compilers (Quintus, SICStus, Ciao, BinProlog,
etc.), and the ``delay technique'' (applying negation as failure only
\emph{when} the variables of the negated goal become ground, which is
sound but incomplete due to the possibility of floundering), which is
present in Nu-Prolog, G\"odel, and Prolog systems that implement
delays (most of the above).

Of all the proposals, constructive negation \cite{Chan1,Chan2} (that
we will call \emph{classical} constructive negation) is probably the
most promising because it has been proven to be sound and complete,
and its semantics is fully compatible with Prolog's. Constructive
negation was, in fact, announced in early versions of the Eclipse
Prolog compiler, but was removed from the latest releases.  The
reasons seem to be related to some technical problems with the use of
coroutining (risk of floundering) and the management of constrained
solutions.


The goal of this paper is to give an algorithmic description of
constructive negation, i.e. explicitly stating the details needed for
an implementation. We also intend to discuss the pragmatic ideas
needed to provide a concrete and real implementation. Early results
for a concrete implementation extending the \ciao\ Prolog compiler are
presented.  We assume some familiarity with constructive negation
techniques and Chan's papers.

The remainder of the paper is organized as follows. Section
\ref{constructive} details our constructive negation algorithm. It
explains how to obtain the $frontier$ of a goal (Section
\ref{frontier}), how to prepare the goal for negation (Section
\ref{preparation}) and, finally, how to negate the goal (Section
\ref{negation}). Section \ref{implementation} discusses implementation
issues: code expansion (Section \ref{expansion}), required disequality
constraints (Section \ref{disequality}), optimizations (Section
\ref{optimization}), examples (Section \ref{examples}) and some
experimental results (Section \ref{results}).  Finally, we conclude
and outline some future work.
 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%   CONSTRUCTIVE NEGATION   %%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Constructive Negation}
\label{constructive}

Most of the papers addressing constructive negation deal with semantic
aspects. In fact, only the original papers by Chan gave some hints
about a possible implementation based on coroutining, but the
technique was only outlined. When we tried to reconstruct this
implementation we came across several problems, including the
management of constrained answers and floundering (which appears to be
the main reason why constructive negation was removed from recent
versions of Eclipse). It is our belief that this problems cannot be
easily and efficiently overcome. Therefore, we decided to design an
implementation from scratch.  One of our additional requirements is
that we want to use a standard Prolog implementation (to be able to
reuse thousands of existing Prolog lines and maintain their
efficiency), so we will avoid implementation-level manipulations that
would delay simple programs without negations.

%% This
%% is coherent with the usual Prolog definition of \naf:

%% \begin{verbatim}
%% naf (P):- P, !, fail.
%% naf (P).
%% \end{verbatim}

We start with the definition of a frontier and how it can be managed
to negate the respective formula.

%%%%%%%%% FRONTIER  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Frontier}
\label{frontier}

Firstly, we present Chan's definition of frontier (we actually owe the formal
definition to Stuckey \cite{Stuckey95}).

\begin{definition}{\em Frontier}

A frontier of a goal $G$ is the disjunction of a finite set of nodes
in the derivation tree such that every derivation of $G$ is either
finitely failed or passes through exactly one {\em frontier node}.
\end{definition}

What is missing is a method to generate the frontier. So far we have
used the simplest possible frontier: the frontier of depth 1 obtained
by taking all the possible single SLD resolution steps. This can be
done by a simple inspection of the clauses of the
program\footnote{Nevertheless, we plan to improve the process by using
abstract interpretation and detecting the degree of evaluation of a
term that the execution will generate.}.  Additionally, built-in based
goals receive a special treatment (moving conjunctions into
disjunctions, disjunctions into conjunction, eliminating double
negations, etc.)

\begin{definition}{\em Depth-one frontier}

    \begin{itemize} 

\item If $G \equiv (G_1;G_2) $ then $Frontier(G) \equiv$
$Frontier(G_1) \vee Frontier(G_2)$.

\item If $G \equiv (G_1,G_2) $ then $Frontier(G) \equiv$
  $Frontier(G_1) \wedge Frontier(G_2)$ and then we have to apply
  DeMorgan's distributive property to retain the disjunction
  of conjunctions format.
  
\item If $G \equiv p( \overline{X}) $ and 
  predicate $p/m$ is defined by N clauses:

$
~~~~~~~~~~p( \overline{X}^1):- C_1'. \\
~~~~~~~~~~p( \overline{X}^2):- C_2'. \\
~~~~~~~~~~\ldots \\
~~~~~~~~~~p( \overline{X}^3):- C_N'. \\
$

The frontier of the goal has the format: $Frontier(G) \equiv \{C_1
\vee C_2 \vee \ldots \vee C_N\}$, where each $C_i$ is the union of the
conjunction of subgoals $C_i'$ plus the equalities that are needed to
unify the variables of $\overline{X}$ and the respective terms of
$\overline{X}^i$.

    \end{itemize}

\end{definition}

\noindent
Consider, for instance, the following code:

\begin{verbatim}
odd(s(0)).
odd(s(s(X))) :- odd(X).
\end{verbatim}

The frontier for the goal $odd (Y)$ is as follows:

\[Frontier(odd(Y)) = \{ ( Y=s(0) ) \vee ( Y=s(s(X)) \wedge odd(X) ) \} \] 

To get the negation of $G$ it suffices to negate the frontier
formula. This is done by negating each component of the disjunction of
all implied clauses (that form the frontier) and combining the
results.


The solutions of $cneg(G)$ are the solutions of the combination
(conjunction) of one solution of each of the N conjunctions
$C_i$. Now we are going to explain how to negate a single
conjunction $C_i$. This is done in two phases: \emph{Preparation} and
\emph{Negation of the formula}.



%%%%%%%% PREPARATION %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Preparation}
\label{preparation}

% Comparar con Chan

Before negating a conjunction obtained from the frontier, we have to
simplify, organize, and normalize this conjunction:

\begin{itemize}

\item {\bf Simplification of the conjunction}. If one of the terms of
$C_i$ is trivially equivalent to $true$ (e.g. $X=X$), we can eliminate
this term from $C_i$. Symmetrically,if one of the terms is trivially
$fail$ (e.g. $X \neq X$), we can simplify $C_i \equiv fail$. The
simplification phase can be carried out during the generation of
frontier terms.

\item {\bf Organization of the conjunction}. Three groups are created
containing the components of $C_i$, which are divided into equalities
($\overline{I}$), disequalities ($\overline{D}$), and other subgoals
($\overline{R}$).  Then, we get $C_i \equiv \overline{I} \wedge
\overline{D} \wedge \overline{R}$.
  
\item {\bf Normalization of the conjunction}. Let us classify the
variables in the formula. The set of variables of the
goal is called $GoalVars$. The set of free variables of $\overline{R}$
is called $RelVars$.

    \begin{itemize}


       \item {\bf Elimination of redundant variables and
       equalities}. If $I_i \equiv X = Y$, where $Y \not\in GoalVars$,
       then we now have the formula $ ( I_1 \wedge \ldots \wedge
       I_{i-1} \wedge I_{i+1} \wedge \ldots \wedge I_{NI} \wedge
       \overline{D} \wedge \overline{R}~) \sigma $, where $ \sigma = \{
       Y / X \}$, i.e. the variable $Y$ is substituted by $X$ in the
       entire formula. 
       \item {\bf Elimination of irrelevant disequalities}. $ImpVars$
       is the set of variables of $GoalVars$ and the variables that
       appear in $\overline{I}$. The disequalities $D_i$ that contain
       any variable that was neither in $ImpVars$ nor in $RelVars$ are
       irrelevant and should be eliminated.

    \end{itemize}

 \end{itemize}

 

%%%%%%%% NEGATION OF THE FORMULA %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Negation of the formula}
\label{negation}

It is not feasible, to get all solutions of $C_i$ and to negate their
disjunction because $C_i$ can have an infinite number of solutions. So,
we have to use the classical constructive negation algorithm.

We consider that $ExpVars$ is the set of variables of $\overline{R}$
that are not in $ImpVars$, i.e. $RelVars$, except the variables of
$\overline{I}$ in the normalized formula.
\medskip

\noindent
{\em First step: {\bf Division of the formula}}

\noindent
$C_i$ is divided into:

\[C_i \equiv \overline{I} \wedge
        \overline{D}_{imp} \wedge \overline{R}_{imp} \wedge
        \overline{D}_{exp} \wedge \overline{R}_{exp} \]

\noindent
where $\overline{D}_{exp}$ are the disequalities in $\overline{D}$
with variables in $ExpVars$ and $\overline{D}_{imp}$ are the other
disequalities, $\overline{R}_{exp}$ are the goals of $\overline{R}$
with variables in $ExpVars$ and $\overline{R}_{imp}$ are the other
goals, and $\overline{I}$ are the equalities.

Therefore, the constructive negation of the divided formula is: \\

$~~~~~~~~~~~~~~~~~~~~\neg~C_i \equiv \neg~\overline{I} \vee $ \\
$~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~(\overline{I} \wedge \neg~\overline{D}_{imp}) \vee  $ \\
$~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~(\overline{I} \wedge \overline{D}_{imp}  \wedge \neg~\overline{R}_{imp}) \vee $ \\
$~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~( \overline{I} \wedge \overline{D}_{imp} \wedge \overline{R}_{imp} \wedge \neg~(\overline{D}_{exp} \wedge \overline{R}_{exp})) $ \\

\noindent
It is not possible to separate $\overline{D}_{exp}$ and
$\overline{R}_{exp}$ because they contain free variables and
they cannot be negated separately. The answers of the negations
will be the answers of the negation of the equalities, the answers of
the negation of the disequalities without free variables, the answers
of the negation of the subgoals without free variables and the answers
of the negation of the other subgoals of the conjunctions (the ones
with free variables). Each of them will be obtained as follows:
\medskip

\noindent
{\em Second step: {\bf Negation of subformulas}}

        \begin{itemize}

           \item {\bf Negation of $\overline{I}$}. We have $\overline{I}
           \equiv I_1 \wedge \ldots \wedge I_{NI} \equiv$ \[ \exists~
           \overline{Z}_1~ X_1 = t_1 \wedge \ldots \wedge \exists~
           \overline{Z}_{NI}~ X_{NI} = t_{NI} \] where
           $\overline{Z}_i$ are the variables of the equality $I_i$ that
           are not included in $GoalVars$ (i.e. that are not quantified
           and are therefore free variables). When we negate this
           conjunction of equalities we get the constraint 
                \[
           \underbrace{\forall~ \overline{Z}_1~ X_1 \neq t_1} _{\neg~
           I_1} \vee \ldots \vee \underbrace{\forall~
           \overline{Z}_{NI}~ X_{NI} \neq t_{NI} } _{\neg~ I_{NI}}
           \equiv %\] 
%                \[ 
           \bigvee_{i=1}^{NI} \forall~ \overline{Z}_i X_i
           \neq t_i \] 
           This constraint is the first answer of the
           negation of $C_i$ that contains $NI$ components.

           \item {\bf Negation of $\overline{D}_{imp}$}. If we have
           $N_{D_{imp}}$ disequalities $\overline{D}_{imp} \equiv D_1
           \wedge \ldots \wedge D_{N_{D_{imp}}}$ where $ D_i \equiv
           \forall~ \exists~ \overline{Z}_i ~\overline{W}_i ~  Y_i
           \neq s_i$ where $Y_i$ is a variable of $ImpVars$, $s_i$ is
           a term without variables in $ExpVars$, $\overline{W}_i$ are
           universally quantified variables that are neither in the
           equalities \footnote{There are, of course, no universally
           quantified variables in an equality}, nor in the other
           goals of $\overline{R}$ because otherwise $\overline{R}$
           would be a disequality of $\overline{D}_{exp}$. Then we
           will get $N_{D_{imp}}$ new solutions with the format: \\

           $\overline{I} \wedge \neg~ D_1 $ \\ 
           $\overline{I} \wedge
           D_1 \wedge \neg~ D_2 $ \\ 
           $\ldots $ \\ 
           $\overline{I} \wedge
           D_1 \wedge \ldots \wedge D_{N_{D_{imp}}-1} \wedge \neg~
           D_{N_{D_{imp}}}$ \\ 

           where $ \neg~ D_i \equiv \exists~
           \overline{W}_i~ Y_i = s_i$. The negation of a universal
           quantification turns into an existential quantification and
           the quantification of free variables of $\overline{Z}_i$
           gets lost, because the variables are unified with the evaluation of
           the equalities of $\overline{I}$. Then, we will get
           $N_{D_{imp}}$ new answers.


           \item {\bf Negation of $\overline{R}_{imp}$}. If we have
           $N_{R_{imp}}$ subgoals $\overline{R}_{imp} \equiv R_1
           \wedge \ldots \wedge R_{N_{R_{imp}}}$. Then we will get
           new answers from each of the conjunctions: \\

           $\overline{I} \wedge \overline{D}_{imp} \wedge \neg~ R_1 $ \\ 
           $\overline{I} \wedge \overline{D}_{imp} \wedge
           R_1 \wedge \neg~ R_2 $ \\ 
           $\ldots $ \\ 
           $\overline{I} \wedge \overline{D}_{imp} \wedge
           R_1 \wedge \ldots \wedge R_{N_{R_{imp}}-1} \wedge \neg~
           R_{N_{R_{imp}}}$ \\ 

           where $ \neg~ R_i \equiv cneg(R_i)$. Constructive negation
           is again applied over $R_i$ recursively using this
           operational semantics.


           \item {\bf Negation of $\overline{D}_{exp} \wedge
           \overline{R}_{exp}$}. This conjunction cannot be disclosed
           because of the negation of $ \exists~ \overline{V}_{exp}~
           \overline{D}_{exp} \wedge \overline{R}_{exp}$, where
           $\overline{V}_{exp}$ gives universal quantifications:\\
           $\forall~ \overline{V}_{exp}~ cneg(\overline{D}_{exp}
           \wedge \overline{R}_{exp})$. The entire constructive
           negation algorithm must be applied again. Note that the new
           set $GoalVars$ is the former set $ImpVars$. Variables of
           $\overline{V}_{exp}$ are considered as free variables. When
           solutions of $cneg(\overline{D}_{exp} \wedge
           \overline{R}_{exp})$ are obtained some can be rejected:
           solutions with equalities with variables in
           $\overline{V}_{exp}$. If there is a disequality with any of
           these variables, e.g. $V$, the variable will be universally
           quantified in the disequality.  This is the way to negate
           the negation of a goal, but there is a detail that was not
           considered in former approaches and that is necessary to
           get a sound implementation: the existence of universally
           quantified variables in $\overline{D}_{exp} \wedge
           \overline{R}_{exp}$ by the iterative application of the
           method.  So, what we are really negating is a subgoal of
           the form: $ \exists~ \overline{V}_{exp}~ \overline{D}_{exp}
           \wedge \overline{R}_{exp}$.  Here we will provide the last
           group of answers that come from:

           \[\overline{I} \wedge \overline{D}_{imp}
           \wedge \overline{R}_{imp} \wedge \forall~
           \overline{V}_{exp}~ \neg~(\overline{D}_{exp} \wedge
           \overline{R}_{exp})\]

         \end{itemize}


    

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%  IMPLEMENTATION ISSUES  %%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Implementation Issues}
\label{implementation}

Having described the theoretical algorithm, including important
details, we now discuss important aspects for a practical
implementation, including how to compute the frontier and manage
answer constraints.

%%%%%%%% CODE EXPANSION  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Code Expansion}
\label{expansion}

The first issue is how to get the frontier of a goal. It is possible
to handle the code of clauses during the execution thanks to the Ciao
package system \cite{ciao-modules-cl2000}, which allows the code to be
expanded at run time. The expansion is implemented in the $cneg.pl$
package which is included in the declaration of the module that is
going to be expanded (i.e. where there are goals that are negations).

%% A simple example would be the module $mod1.pl$ that exports the
%% predicate $odd/1$ and the predicate $not\_odd/1$ that, semantically, is
%% the negation of $odd/1$:
%% \begin{verbatim}
%% :- module(mod1,[odd/1,not_odd/1],[cneg]).

%% odd(s(0)).
%% odd(s(s(X))) :- odd(X).

%% not_odd(X) :- cneg(odd(X)).
%% \end{verbatim}
%% The loading of the $cneg.pl$ package means that the compiler works
%% with an expanded code added to the previous code:
%% \begin{verbatim}
%% stored_clause(odd(s(0)),[]).
%% stored_clause(odd(s(s(X))),[odd(X)]).
%% \end{verbatim}
%% where information about code structure is stored to be used by the
%% negation algorithm. Now, the execution is able to compute the frontier
%% we described above $\{ ( Y=s(0) ) \vee ( Y=s(s(X)) \wedge odd(X) ) \}$

Note that a similar, but less efficient, behaviour can be emulated
using metaprogramming facilities, available in most Prolog compilers.
 

%%%%%%%% DISEQUALITY CONSTRAINTS  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Disequality constraints}
\label{disequality}

An instrumental step for managing negation is to be able to handle
disequalities between terms such as $t_1 \neq t_2$.  The typical
Prolog resources for handling these disequalities are limited to the
built-in predicate {\tt /== /2}, which needs both terms to be ground
because it always succeeds in the presence of free variables.  It is
clear that a variable needs to be bound with a disequality to achieve
a ``constructive'' behaviour.  Moreover, when an equation $X =
t(\overline{Y})$ is negated, the free variables in the equation must
be universally quantified, unless affected by a more external
quantification, i.e. $\forall~ \overline{Y}~X \neq t(\overline{Y})$ is
the correct negation.  As we explained in \cite{SusanaPADL2000}, the
inclusion of disequalities and constrained answers has a very low
cost. It incorporates negative normal form constraints instead of
bindings and the decomposition step can produce disjunctions.

%%  Our
%% proposal for normal form constraints is:

%% %\vspace{-20pt}
%% \[ \underbrace{\bigwedge_i (X_i = t_i)}_{\mbox{positive information}} \wedge~~~~ ( \underbrace{\bigwedge_j \forall~ \overline{Z}_j^1~(Y_j^1 \neq s_j^1) \vee \ldots \vee \bigwedge_l \forall~ \overline{Z_l}^n~(Y_l^n \neq s_l^n) ) }_{\mbox{negative information}} \]
%% %\vspace{-20pt}

%% \noindent
%% where each $X_i$ appears only in $X_i = t_i$, no $s_k^r$ is $Y_k^r$
%% and the universal quantification could be empty (leaving a simple
%% disequality).

%% It is easy to see that some normalization rules can be defined for a
%% normal form formula from any initial formula. The redefinition of the
%% unification algorithm to manage constrained variables is also a simple
%% exercise.

%% \cite{Moreno1} introduces this very compact way to represent a normal
%% form constraint.  Chan's representation uses only disjunctions and
%% they are dealt with by means of backtracking. The main advantage of our
%% normal form is that the search space is drastically reduced.

%% To include disequalities into a Prolog compiler, we need to just
%% reprogram unification. This can be done using attributed variables
%% \cite{Carlsson} (available in several Prolog versions, e.g. in Sicstus
%% Prolog, or in Eclipse, where they are called meta-structures). These
%% variables allow us to keep information associated with each variable
%% (in an attibute that is a term) during the unification, which can be
%% used to dynamically control the constraints.

%% %% Attributed variables are variables with an associated attribute.  which
%% %% is a term. Each variable has an associated data structure, containing a
%% %% normal form constraint: a list of lists of pairs (variable, term). They
%% %% behave like ordinary variables, except that the programmer can supply
%% %% code for unification, printing facilities and memory management. In
%% %% our case, the printing facility is used to show constrained
%% %% answers. The main task is to provide a new unification code.

%% For the unification of a variable $X$ with a term $t$, there are three
%% possible cases (up to commutativity):

%% %\vspace{-5pt}
%% \begin{enumerate}
%% %\addtolength{\itemsep}{-8pt}

%%    \item $X$ is a free variable and $t$ is not a variable with a
%%    negative constraint: just bind $X$ to $t$,

%%    \item $X$ is a free variable or bound to a term $t'$ and $t$ is a
%%    variable $Y$ with a negative constraint: check whether $X$ (or,
%%    equivalently, $t'$) satisfies the constraint associated with $Y$.
%%    A conveniently defined predicate {\tt satisfy} is used for this
%%    purpose,

%%    \item $X$ is bound to a term $t'$ and $t$ is a term (or a variable
%%    bound to a term): use the classical unification algorithm.

%% \end{enumerate}
%% %\vspace{-5pt}

A Prolog predicate {\tt =/= /2} \cite{SusanaPADL2000} has been
defined, used to check disequalities, similarly to explicit
unification ({\tt =}). Each constraint is a disjunction of
conjunctions of disequalities. When a universal quantification is used in a disequality (e.g.,
$\forall Y~ X \neq c(Y)$), the new constructor {\tt fA}$/1$ is used
(e.g., {\tt X / c(fA(Y)))}.  

%% The first list is used to represent
%% disjunctions while the internal list represents the conjunction of
%% disequalities.

%% Let us show some examples involving variable $X$ where we show the
%% corresponding attribute that represents the constraint of each
%% subgoal:

%% %\hspace{-0.5cm}
%% \begin{center}
%% \begin{small}
%% \begin{tabular}{lll}
%% SUBGOAL & ATTRIBUTE & CONSTRAINT \\
%% \hline\hline
%% \ \\
%% {\tt not\_member(X,[1,2,3])}   &  $X=/=1,X=/=2,X=/=3$  & $X \neq 1 \wedge X \neq 2 \wedge X \neq 3$\\
%% {\tt member(X,[1,2,3]),X=/=2} &  $X=/=1,X=/=3$       & $X \neq 1 \wedge X \neq 3$\\
%% {\tt member(X,[1]), X=/=1}     &  {\tt fail}          & $false$ \\
%% {\tt X =/= 4}                   & $X=/=4$            & $X \neq 4$ \\
%% {\tt X =/= 4; X=/=5}            & $X=/=4~ ;~ X/5$     & $X \neq 4 \vee X \neq 5$ \\
%% {\tt X =/= 5; (X=/=6, X=/=Y)}   & $X=/=5 ~; ~ (X=/=6, X=/=Y)$ & $X \neq 5 \vee (X \neq 6 \wedge X \neq Y)$\\
%% {\tt forall([Y], X =/= s(Y))}     & $X=/=s(fA(Y)$  & $\forall Y. X \neq Y$ \\
%% %{\tt (member (X,[0,s(0),s(s(0))]),} &                  & \\
%% %{\tt forall (Y, X =/= s (Y)) )}     & $0$              & $X = 0$
%% \end{tabular}
%% \end{small}
%% \end{center}



%%%%%%%% OPTIMIZATION  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Optimizing the algorithm and the implementation}
\label{optimization}

Our constructive negation algorithm and the implementation techniques
admit some additional optimizations that can improve the runtime
behaviour of the system. Basically, the optimizations rely on the
compact representation of information, as well as the early detection
of successful or failing branches.

\noindent
{\bf Compact information}. In our system, negative information is
represented quite compactly, providing fewer solutions from the
negation of $\overline{I}$. The advantage is twofold. On the one hand
constraints contain more information and failing branches can be
detected earlier (i.e. the search space could be smaller). On the
other hand, if we ask for all solutions using backtracking, we are
cutting the search tree by offering all the solutions together in a
single answer. For example, we can offer a simple answer for the
negation of a predicate $p$ (the code for $p$ is skipped):

\begin{verbatim}
?- cneg(p(X,Y,Z,W)).

(X=/=0, Y=/=s(Z)) ; (X=/=Y) ; (X=/=Z) ; 
(X=/=W) ; (X=/=s(0), Z=/=0) ? ;
no
\end{verbatim}

\noindent
(which is equivalent to $ (X \neq 0 \wedge Y\neq s(Z)) \vee X \neq Y
\vee X \neq Z \vee X \neq W \vee X \neq s(0) \vee Z \neq 0$),
instead of returning six answers upon backtracking:
\begin{verbatim}
?- cneg(p(X,Y,Z,W)).

X=/=0, Y=/=s(Z) ? ;
X=/=Y ? ;
X=/=Z ? ;
X=/=W ? ;
X=/=s(0) ? ;
Z=/=0 ? ;
no
\end{verbatim}


\noindent
{\bf Pruning subgoals}. The frontiers generation search tree can be
cut with a double action over the ground subgoals: removing the
subgoals whose failure we are able to detect early on, and simplifying the
subgoals that can be reduced to true. Suppose  we have a predicate $p/2$
defined as
\begin{verbatim}
p(X,Y):- greater(X,Y),
         q(X,Y,Z),
         r(Z).
\end{verbatim}
\noindent
where $q/3$ and $r/1$ are predicates defined by several
clauses with a complex computation. To negate
the goal $p(s(0),s(s(0)))$, its frontier is computed:

$$Frontier(p(s(0),s(s(0)))) \equiv $$
$${ X=s(0) \wedge Y=s(s(0)) \wedge
  greater(X,Y) \wedge q(X,Y,Z) \wedge r(Z) } \equiv $$
$${ greater(s(0),s(s(0))) \wedge q(s(0),s(s(0)),Z) \wedge r(Z) } \equiv $$
$${ fail  \wedge q(s(0),s(s(0)),Z) \wedge r(Z) } \equiv $$
$$fail $$

The next step is to expand the code of the subgoals of the frontier to
the combination (disjunction) of the code of all their clauses, and the
result will be a very complicated and hard to check frontier.
However, the process is optimized by evaluating ground terms. In this
case, $greater(s(0),s(s(0))$ fails and, therefore, it is not necessary to
continue with the generation of the frontier, because the result is
reduced to fail (i.e. the negation of $p (s(0), s(s(0)))$ will be
trivially true). The opposite example is a simplification:

$$Frontier(p(s(s(0)),s(0))) \equiv $$
$${ X=s(s(0)) \wedge Y=s(0) \wedge greater(X,Y) \wedge q(X,Y,Z) \wedge
  r(Z) } \equiv $$
$${ greater(s(s(0)),s(0)) \wedge q(s(s(0)),s(0),Z) \wedge r(Z) } \equiv
$$
$${ true \wedge q(s(s(0)),s(0),Z) \wedge r(Z) } \equiv $$
$${ q(s(s(0)),s(0),Z) \wedge r(Z) } $$

\noindent
{\bf Constraint simplification}. During the whole process for negating
a goal,the frontier variables are constrained. In cases where the
constraints are satisfiable, they can be eliminated and where the
constraints can be reduced to fail, the evaluation can be stopped with
result \emph{true}.
 
We focus on the negative information of a normal form constraint $F$:
\[ F \equiv  \bigvee_i\bigwedge_j \forall~ \overline{Z}_j^i~(Y_j^i \neq s_j^i) \]
Firstly, the Prenex form \cite{Shoenfield} can be obtained by
extracting the universal variables with different names to the head of
the formula, applying logic rules:
\[ F \equiv \forall \overline{x} \bigvee_i\bigwedge_j (Y_j^i \neq s_j^i) \]
\noindent
and using the distributive property:
\[ F \equiv \forall \overline{x} \bigwedge_k\bigvee_l (Y_l^k \neq s_l^k) \]
The formula can be separated into subformulas that are simple
disjunctions of disequalities :
\[ F \equiv \bigwedge_k \forall \overline{x} \bigvee_l (Y_l^k \neq s_l^k) \equiv F_1 \wedge ... \wedge F_n\]
Each single formula $F_k$ can be evaluated. The first step will be to
substitute the existentially quantified variables (variables that do not
belong to $\overline{x}$) by Skolem constants $s^i_j$ that will keep
the equivalence without losing generality:
\[ F_k \equiv \forall \overline{x} \bigvee_l ( Y_l^k \neq s_l^k ) \equiv \forall \overline{x} \bigvee_l ( Y_{Sk l}^k \neq s_{Sk l}^k )  \]
Then it can be transformed into:
\[ F_k \equiv  \neg \exists ~ \overline{x} \neg ( \bigvee_l (Y_{Sk l}^k \neq s_{Sk l}^k) ) \equiv \neg Fe_K \]
The meaning of $F_k$ is the negation of the meaning of $Fe_k$;
\[ Fe_k \equiv \exists ~ \overline{x} \neg ( \bigvee_l (Y_{Sk l}^k \neq s_{Sk l}^k)) \] 
Solving the negations, the result is obtained through simple unifications of the variables of $\overline{x}$:

\[ Fe_k  \equiv \exists ~ \overline{x} \bigwedge \neg (Y_{Sk l}^k \neq s_{Sk l}^k)  \equiv \exists ~ \overline{x} \bigwedge (Y_{Sk l}^k = s_{Sk l}^k)  \]

        Therefore, we get the truth value of $F_k$ from the
        negation of the value of $Fe_k$ and, finally, the value of $F$ is
        the conjunction of the values of all $F_k$. If $F$
        succeeds, then the constraint is removed because it is redundant
        and we continue with the negation process. If it fails, then
        the negation directly succeeds.

%%%%%%%%%%%%%%%%%%%%%%%% RESULTS %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Experimental results}
\label{results}

Our prototype is a simple library that is added to the set of
libraries of Ciao Prolog. Indeed, it is easy to port the library to
other Prolog compilers. The only requirement is that attributed
variables should be available.

This section reports some experimental results from our prototype
implementation.  First of all, we show the behaviour of the
implementation in some simple examples.

%%%%%%%% EXAMPLES  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Examples}
\label{examples}

The interesting side of this implementation is that it returns
constructive results from a negative question. Let us start with a
simple example involving predicate $boole/1$.

\begin{minipage}{2in}
\begin{verbatim}
boole(0).
boole(1).
\end{verbatim}
\end{minipage}
\begin{minipage}{2in}
%\rnode{A}{}
\begin{verbatim} 

    ?- cneg(boole(X)).
    X=/=1, X=/=0 ? ;
    no
\end{verbatim} 
%\rnode{B}{}
%\ncline{A}{B}
\end{minipage}\\

Another simple example obtained from \cite{Stuckey95} gives us the
following answers:

\begin{minipage}{2in}
\begin{verbatim}
p(a,b,c).
p(b,a,c).
p(c,a,b).

proof1(X,Y,Z):-
    X =/= a,
    Z = c,
    cneg(p(X,Y,Z)).
\end{verbatim}
\end{minipage}
\begin{minipage}{2in}
\begin{verbatim} 

   ?- proof1(X,Y,Z).

   Z = c,
   X=/=b, X=/=a ? ;

   Z = c,
   Y=/=a, X=/=a ? ;

   no
\end{verbatim} 
\end{minipage}\\

\cite{Stuckey95} contains another example showing how a constructive
answer ($\forall T ~ X \neq s(T)$) is provided for the negation of an
undefined goal in Prolog:

\begin{minipage}{2in}
\begin{verbatim}

p(X):- X = s(T), q(T).

q(T):- q(T).

r(X):- cneg(p(X)).
\end{verbatim}
\end{minipage}
\begin{minipage}{2in}
\begin{verbatim} 
   ?- r(X).

   X=/=s(fA(_A)) ?

   yes
\end{verbatim} 
\end{minipage}\\



Notice that if we would ask for a second answer, then it will loop
according to the Prolog resolution. An example with an infinite number
of solutions is more interesting.

\begin{minipage}{1.5in}
\begin{verbatim}
positive(0). 
positive(s(X)):-
        positive(X).  
\end{verbatim}
\end{minipage} 
\begin{minipage}{2.5in}
%\rnode{A}{}
\begin{verbatim} 

    ?- cneg(positive(X)).

    X=/=s(fA(_A)), X=/=0 ? ;
    X = s(_A), 
    (_A=/=s(fA(_B)), _A=/=0) ? ;
    X = s(s(_A)), 
    (_A=/=s(fA(_B)), _A=/=0) ? ;
    X = s(s(s(_A))),
    (_A=/=s(fA(_B)), _A=/=0) ? 
    yes
\end{verbatim} 
%\rnode{B}{}
%\ncline{A}{B}
\end{minipage}

%% \begin{minipage}{1.5in}
%% \begin{verbatim}
%% number(0).
%% number(s(X)):-
%%         number(X).

%% greater(s(X),0):-
%%         number(X).
%% greater(s(X),s(Y)):-
%%         greater(X,Y).
%% \end{verbatim}
%% \end{minipage}
%% \begin{minipage}{2.5in}
%% %\rnode{A}{}
%% \begin{verbatim} 
%%     ?- cneg(greater(X,Y)).

%%     Y=/=0, Y=/=s(fA(_A)) ? ;

%%     (Y=/=s(fA(_A))), 
%%     (X=/=s(fA(_B))) ? ;

%%     X = s(_A),Y = 0,
%%     (_A=/=s(fA(_B)),_A=/=0) ? ;

%%     X = s(s(_A)),Y = 0,
%%     (_A=/=s(fA(_B)),_A=/=0) ? ;

%%     X = s(s(s(_A))),Y = 0,
%%     (_A=/=s(fA(_B)),_A=/=0) ? ;

%%     X = s(s(s(s(_A)))),Y = 0,
%%     (_A=/=s(fA(_B)),_A=/=0) ? 

%%     yes
%% \end{verbatim} 
%% %\rnode{B}{}
%% %\ncline{A}{B}
%% \end{minipage}

\subsection{Implementation measures}

We have firstly measured the execution times in milliseconds for the
above examples when using negation as failure ($naf/1$) and
constructive negation ($cneg/1$). A `-' in a cell means that negation
as failure is not applicable. Some goals were executed a number of
times to get a significant measurement. All of them were made using
\ciao\ Prolog\footnote{The negation system is coded as a library
module (``package'' \cite{ciao-modules-cl2000}), which includes the
respective syntactic and semantic extensions (i.e. Ciao's attributed
variables). Such extensions apply locally within each module which
uses this negation library.} 1.5 on a Pentium II at 350 MHz. The
results are shown in Table~\ref{table}. We have added a first column
with the runtime of the evaluation of the positive goal that is
negated in the other columns and a last column with the ratio that
measures the speedup of the \naf\ technique w.r.t. constructive
negation.

Using {\bf naf} instead of {\bf cneg} results in small ratios around
1.06 on average for ground calls with few recursive calls. So, the
possible slow-down for constructive negation is not so high as we
might expect for these examples. Furthermore, the results are rather
similar. But the same goals with data that involve many recursive
calls yield ratios near 14.69 on average w.r.t {\bf naf},
increasing exponentially with the number of recursive calls. There
are, of course, many goals that cannot be negated using the \naf\
technique and that are solved using constructive negation.

\input{table} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%% FINITE CONSTRUCTIVE NEGATION  %%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Finite Constructive Negation}
\label{cneg}

The problem with the constructive negation algorithm is of course
efficiency. It is the price that it has to be paid for a powerful
mechanism that negates any kind of goal. Thinking of Prolog programs,
many goals have a finite number of solutions. There is a
simplification of the constructive negation algorithm that we use to
negate these goals. It is very simple in the sense that if we have a
goal $\neg G$ where the solution of the positive subgoal $G$ is a set
of $n$ solutions like $\{S_1, S_2,...,S_n\}$, then we can consider
these equivalences:

$$G \equiv S_1 \vee S_2 \vee ... \vee S_n $$
$$\neg G \equiv \neg(S_1 \vee S_2 \vee ... \vee S_n) $$
$$\neg G \equiv (\neg S_1 \wedge \neg S_2 \wedge ... \wedge \neg S_n)$$

Of course, these solutions are a conjunction of unifications
(equalities) and disequality constraints. As described in section
\ref{disequality}, we know how to handle and negate this kind of
information. The implementation of the predicate $cnegf/1$ is
something akin to

\begin{verbatim}
cnegf(Goal):-
  varset(Goal,GVars), % Getting variables of the Goal
  setof(GVars,Goal,LValores),!, % Getting the solutions
  cneg_solutions(GVars,LValores). % Negating solutions

cnegf(_Goal). % Without solutions, the negation succeeds
\end{verbatim}

\noindent
where $cneg\_solutions/2$ is the predicate that negates the disjunction
of conjunctions of solutions of the goal that we are negating. It
works as described in section \ref{negation}, but it is simpler,
because here we are only negating equalities and disequalities.

We get the set of variables, $GVars$, of the goal, $Goal$, that we
want to negate (we use the predicate $varset/2$). Then we use the
$setof/3$ predicate to get the values of the variables of $GVars$
 for each solution of $Goal$. For example, if we want to
evaluate $cnegf(boole(X))$, then we get $varset(boole(X),[X])$,
$setof([X],boole(X),[[0],[1]])$ (i.e. $X=0 \vee X=1$) and
$cneg\_solutions/2$ will return $X \neq 0
\wedge X \neq 1$. 

If we have the goal $p(X,Y)$, which, has two solutions $X=a,~Y=b$ and
$X=c,~Y=d$, then, in the evaluation of $cnegf(p(X,Y))$, we will get
$varset(p(X,Y),[X,Y])$, $setof([X,Y],p(X,Y),[[a,b],[c,d]])$
(i.e. $(X=a \wedge Y=b) \vee (X=c \wedge Y=d)$) and \\$cneg\_solutions/2$
will return the four solutions $(X \neq a \wedge X \neq c) \vee (X
\neq a \wedge Y \neq d) \vee (Y \neq b \wedge X \neq c) \vee (Y \neq b
\wedge Y \neq d)$.


%% \subsection{Formal Results}

%% Let us begin with the formal definition of the predicate $cnegf$.

%% \index{cnegf}
%% \begin{definition}[Cnegf]
%% \label{def:cnegf}
%% If $P$ is a normal program, $G$ is a goal with a finite number of
%% solutions that form a maximum $(P,\mathcal{A},R)$-frontier
%% $\{G_1,...,G_n\}$ for $G$ then \emph{cnegf} is a predicate that
%% verifies
%% %
%%  \[ \mathcal{A} \wedge P^* \entails_3 cnegf(G) \iff \neg((\exists
%%     \vecy_1 G_1) \vee ... \vee (\exists \vecy_n G_n)) \]
%% %
%% \noindent
%% where $\vecy_i$ is the set of variables in $G_i$ not in $G$. $\Box$
%% \end{definition}

%% We can easily obtain formal results.
%% \begin{corollary}[Soundness and Completeness of cnegf]
%% \label{cor:soundness_completeness_cnegf}
%% If $P$ is a normal program, and $G$ is a goal with a finite number of
%% solutions then
%% %
%%  \[ \mathcal{A} \wedge P^* \entails_3 
%%     cnegf(G) \iff \neg(G) \]
%% %
%% \end{corollary}

%% \begin{proof}
%% For definition \ref{def:cnegf}, and corollary
%% \ref{cor:soundness_completeness_cneg}. Because $cnegf$ is applied to
%% the particular case where $\{G_1,...,G_n\}$ is the frontier of maximum
%% depth.
%% \end{proof}

%It is the same for $cnegf(G)$ that is a particular case of $cneg(G)$
%where $\{G_1,...,G_n\}$ is the frontier of maximum depth.


\subsection{Analysis of the number of solutions}
\label{cneg:finite_analysis}

The below optimization is very intuitive but, perhaps, the main
problem is to detect when a goal is going to have a finite number of
solutions. To get sound results, we are going to use this technique
(finite constructive negation) just to negate the goals that, we are
sure do not have infinite solutions. So, our analysis is conservative.

We use a combination of two analyses: the non-failure analysis
\cite{Lopez1} and the analysis of upper cost \cite{Lopez2}. Both are
implemented in the Ciao Prolog precompiler
\cite{ciaopp-iclp99-tut}. We test these analyses at compilation time
and then, when possible, we directly execute the optimized version
of constructive negation at compilation time.

If we get for the subgoal $G$ that:
\begin{itemize}
      \item the nonfailure analysis yields ``$G$ does not fails'' and
      \item the cost analysis yields ``$G$ has an upper cost
      inferior to infinite'',
\end{itemize}
\noindent
then we cab be sure about that $G$ has a finite number of solutions.

It is more complicated to check this at execution time although we
could provide a rough approximation. First, we get a maximun number
$N$ of solutions of $G$ (we can use the library predicate
$findnsols/4$) and then we check the number of solutions that we have
obtained. If it is less than $N$, we can assure that $G$ has a finite
number of solutions and otherwise we do not know.


\subsection{Experimental results}

\index{$cnegf/1$}
We have implemented a predicate $cnegf/1$ to negate the disjunction of
all the solutions of its argument. The implementation of this
predicate takes advantage of backtracking to obtain only the
information that we need to get the first answer. Then, if the user
asks for another answer, then the backtracking gets information enough
to provide it. Accordingly, we avoid the complete evaluation of the
negation of all the solutions first time round. We negate the subterms
only when we need to provide the next solution. In this sense, if we
have the goal $G$ (where each $S_i$ is the conjunction of $Ni$
equalities or disequalities)


$$G \equiv S_1 \vee ... \vee S_n $$
$$G \equiv (S_1^1 \wedge...\wedge S_1^{N1}) \vee ... \vee (S_n^1
\wedge...\wedge S_n^{Nn}) $$

\noindent
and we then want to obtain $\neg G$, then we have
$$\neg G \equiv \neg(S_1 \vee S_2 \vee ... \vee S_n) $$
$$\neg G \equiv (\neg S_1 \wedge \neg S_2 \wedge ... \wedge \neg S_n)$$
$$\neg G \equiv \neg(S_1^1 \wedge...\wedge S_1^{N1}) \wedge ... \wedge
\neg (S_n^1 \wedge...\wedge S_n^{Nn}) $$
$$\neg G \equiv (\neg S_1^1 \vee...\vee \neg S_1^{N1}) \wedge
... \wedge ( \neg S_n^1 \vee ...\vee \neg S_n^{Nn}) $$
$$\neg G \equiv (\neg S_1^1 \wedge...\wedge \neg S_n^{1}) \vee
... \vee (\neg S_1^{N1} \wedge...\wedge \neg S_n^{Nn}) $$
\noindent
we begin calculating just the first answer of the negation that will
be $~~(\neg S_1^1 \wedge...\wedge \neg S_n^{1})~~ $, and the rest will
be calculated if necessary using backtracking.

Let us present a simple example:


\begin{minipage}{2in}
\begin{verbatim}

?- member(3,[X,Y,Z]).
X = 3 ? ;
Y = 3 ? ;
Z = 3 ? ;
no

\end{verbatim}
\end{minipage} 
\begin{minipage}{2.5in}
%\rnode{A}{}
\begin{verbatim} 
?- cnegf(member(3,[X,Y,Z])).
X=/=3, Y=/=3, Z=/=3 ?;
no


\end{verbatim} 
%\rnode{B}{}
%\ncline{A}{B}
\end{minipage}

\noindent
We get the symmetric behavior for the negation of the
negation of the initial query
\begin{verbatim}
?- cnegf(cnegf(member(3,[X,Y,Z]))).
X = 3 ? ;
Y = 3 ? ;
Z = 3 ? ;
no
\end{verbatim}

\input{table_cnegf}

We checked some time results in Table~\ref{table_cnegf}. The results
are much more significant for more complicated goals. Indeed, the more
complicated the code of a predicate is, the more inefficient its
classical constructive negation ($cneg$) is. However, finite
constructive negation ($cnegf$) is independent of code
complexity. Finite constructive negation depends on the complexity of
the solutions obtained for the positive goal and, of course, the
number of solutions of this goal.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%% CONCLUSIONS %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\vspace{-1em}
\section{Conclusion and Future Work}
\label{conclusion}
%\vspace{-1em}
After running some preliminary experiments with the classical constructive 
negation technique  following Chan's description, we realized that the
algorithm needed some additional explanations and modifications.

Having given a detailed specification of algorithm in a detailed way
we proceed to provide a real, complete and consistent
implementation. The result, we have reported are very encouraging,
because we have proved that it is possible to extend Prolog with a
constructive negation module relatively inexpensively and overall
without any delay in Prolog programs that are not using this
negation. Nevertheless, it is quite important to address possible
optimizations, and we are working to improve the efficiency of the
implementation. These include a more accurate selection of the
frontier based on the demanded form of argument in the vein of
\cite{Moreno2}). Another possible future work is to incorporate our algorithm
at the WAM machine level.

In any case, we will probably not be able to provide an efficient enough
implementation of constructive negation, because the algorithm is
inherently inefficient.  This is why we do not intend to
use it either for all cases of negation or for negating goals
directly.

Our goal is to design and implement a practical negation operator and
incorporate it into a Prolog compiler.
In~\cite{SusanaPADL2000,SusanaLPAR01} we systematically studied what
we understood to be the most interesting existing proposals: negation
as failure (\naf) \cite{Clark}, use of delays to apply \naf\
securely~\cite{naish:lncs}, intensional
negation~\cite{Barbuti1,Barbuti2}, and constructive negation
\cite{Chan1,Chan2,Drabent,Stuckey,Stuckey95}. As none of them can
satisfy our requirements of completeness and efficiency, we propose to
use a combination of these techniques, where the information from
static program analyzers could be used to reduce the cost of selecting
techniques \cite{SusanaLPAR01}. So, in many cases, we avoid the
inefficiency of classical constructive negation. However, we still
need it because it is the only method that is sound and complete for
all kind of goals. For example, looking at the goals in
Table~\ref{table}, the strategy will obtain all ground negations using
the \naf\ technique and it would only use classical constructive
negation for the goals with variables where it is impossible to use
\naf\ . Otherwise, the strategy will use finite constructive negation
($cnegf$) for the three last goals of Table~\ref{table_cnegf} because
the positive goals have a finite number of solutions.

We are testing the implementation and trying to improve the code, and
our intention is to include it in the next version of Ciao Prolog
\footnote{http://www.clip.dia.fi.upm.es/Software}.
  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%% BIBLIOGRAPHY %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

 \begin{small}

% \linespread{0.80}
    \bibliographystyle{plain} 
    \bibliography{bibliography}

 \end{small}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%  THE END  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

