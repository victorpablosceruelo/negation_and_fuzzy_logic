
@begin{itemize}
@item for program specialization:
@begin{itemize}
 @item @tt{simp} This transformation tries to explote analysis
 information in order to @em{simplify} the program as much as
 possible. It includes optimizations such as abstract executability of
 literals, removal of useless clauses, and unfolding of literals for
 predicates which are defined by just a fact or a single clause with
 just one literal in its body (a @em{bridge}). It also propagates
 failure backwards in a clause as long as such propagation is safe.
 @item @tt{spec} This transformation performs the same optimizations
 as @tt{simp} but it also performs multiple specialization when this
 improves the possibilities of optimization. The starting point for
 this transformation is not a program annotated with analysis
 information, as in the case above, but rather an @em{expanded
						     program} which
 corresponds to the analysis graph computed by multi-variant abstract
 interpretation. A minimization algorithm is used in order to
 guarantee that the resulting program is minimal in the sense that
 further collapsing versions would represent losing opportunities for
 optimization. 
 @item @tt{vers} This transformation has in common with @tt{spec} that
 it takes as starting point the @em{expanded program} which
 corresponds to the analysis graph computed by abstract
 interpretation. However, this transformation performs no
 optimizations and does not minimize the program. As a result, it
 generates the expanded program.
@end{itemize}

@item for partial evaluation:
@begin{itemize}
 @item @tt{codegen} This generates the specialized program resulting
 from partial evaluation, obtained by unfolding goals during analysis. The
 kind of unfolding performed is governed by the @tt{comp\_rule} flag,
 as follows:

   @begin{itemize}
   @item @tt{leftmost} unfolds the leftmost clause literal;
   @item @tt{eval\_builtin} selects for unfolding first builtins which can
         be evaluated;
   @item @tt{local\_emb} tries to select first atoms which do not endanger
         the embedding ordering or evaluable builtins whenever possible;
   @item @tt{jump\_builtin} selects the leftmost goal but can `jump' over
         (ignore) builtins when they are not evaluable. A main difference
         with the other computation rules is that unfolding is performed 
         `in situ', i.e., without reordering the atoms in the clause.
   
   @item @tt{safe\_jb} same as @tt{jump\_builtin} with the difference
 that it only jumps over a call to a builtin iff the call is safe @cite{nonleftmost-lopstr05} (i.e., it is error free, binding insensitive and side effect free).

   @item @tt{bind\_ins\_jb} same as @tt{safe\_jb} with the
   difference that it only jumps over a call to a builtin iff  the call is binding insensitive and side effect free.

    @item @tt{no\_sideff\_jb} same as @tt{bind\_ins\_jb} with the
  difference that it only jumps over a call to a builtin iff it is
  side effect free.  

@end{itemize}

Unfolding is performed continuously on the already unfolded clauses, until a 
condition for stopping the process is satisfied. This condition is stablished
by the local control policy, governed by the @tt{local\_control} flag,
as follows:

   @begin{itemize}
   @item @tt{inst} allows goal instantiation but no actual unfolding is
         performed.
   @item @tt{orig} returns the clauses in the original program for the
         corresponding predicate.
   @item @tt{det} allows unfolding while derivations are deterministic 
         and stops them when a non-deterministic branch is required.
         Note that this may not be terminating.
   @item @tt{det\_la} same as @tt{det}, but with look-ahead. It can perform
          a number of non-deterministic steps in the hope that the computation
          will turn deterministic. This number is determined by flag
          @tt{unf\_depth}.
   @item @tt{depth} always performs the same number of unfolding steps for 
         every call pattern. The number is determined by flag @tt{unf\_depth}.
   @item @tt{first\_sol} explores the SLD tree width-first and keeps on 
         unfolding until a first solution is found. It can be non-terminating.
   @item @tt{first\_sol\_d} same as above, but allows terminating when a
         given depth bound is reached without obtaining any solution.
         The bound is determined by @tt{unf\_depth}.
   @item @tt{all\_sol} tries to generate all solutions by exploring the 
         whole SLD tree. This strategy only terminates if the SLD is finite.
   @item @tt{hom\_emb} keeps on unfolding until the selected atom is
         homeomorphically embedded in an atom previously selected for 
         unfolding.
   @item @tt{hom\_emb\_anc} same as before, but only takes into account 
         previously
         selected atoms which are ancestors of the currently selected atom.
   @item @tt{hom\_emb\_as} same as before, but efficiently implemented
         by using a stack to store ancestors.
   @item @tt{df\_hom\_emb\_as} same as before, but traverses the SLD tree
         on a depth-first fashion (all strategies above use wide-first
         search). This allows better performance.
   @item @tt{df\_tree\_hom\_emb} same as above, but does not use the
         efficient stack-based implementation for ancestors.
   @item @tt{df\_hom\_emb} same as above, but compares with all
         previously selected atoms, and not only ancestors. It is like
         @tt{hom\_emb} but with depth-first traversal.
   @end{itemize}

 @item @tt{global\_control} In order to guarantee termination of the
 partial evaluation process, it is often required to abstract away
 information before unfolding. This is usually known as global
 control. This flag can have the following values:

   @begin{itemize}
   @item @tt{off} unfolds always;
   @item @tt{id} unfolds patterns which are not equal (modulo renaming) to a 
	 formerly analyzed pattern.
   @item @tt{inst} unfolds patterns which are not an instance of a previous
	 pattern.
   @item @tt{hom\_emb} unfolds patterns which are not covered under the
         homeomorphic embedding ordering @cite{Leuschel:SAS98}.
    @item @tt{hom\_emb\_num} same as @tt{hom\_emb},
	but also considers that any number embeds any other number.

 @end{itemize}

   Only @tt{hom\_emb} guarantees termination. However, @tt{id} and
   @tt{inst} are more efficient, and terminating in many practical
   cases.

 @item @tt{arg\_filtering} This transformation removes from program literals
 static values which are not needed any longer in the resulting program.
 This is typically the case when some information is known at compile-time
 about the run-time values of arguments. 

 @item @tt{codegen\_af} This performs @tt{codegen} and @tt{arg\_filtering} 
 in a single traversal of the code. Good for efficiency.
@end{itemize}

@item for code size reduction:
@begin{itemize}
 @item @tt{slicing} This transformation is very useful for debugging
 programs since it isolates those predicates that are reachable from a
 given goal. The goals used are those exported by the module.
 The `slice' being obtained is controlled by the following local control 
 policies (described above): @tt{df\_hom\_emb\_as}, @tt{df\_hom\_emb},
 @tt{df\_tree\_hom\_emb}. It is also necessary to analyze the program
 with any of the currently available analyses for partial evaluation.
 Slicing is also very useful in order to perform other software engineering 
 tasks, such as program understanding, maintenance, specialization, 
 code reuse, etc.
@end{itemize}

@item for program parallelization:

Parallelization is performed by considering goals the execution of which
can be deemed as @em{independent} @cite{sinsi-jlp-short,consind-toplas}
under certain conditions. Parallel expressions (possibly conditional) are
built from such goals, in the following fashions:

@begin{itemize}
 @item @tt{mel} exploits parallel expressions which preserve the ordering
                of literals in the clauses;
 @item @tt{cdg} tries to exploit every possible parallel expression, without
                preserving the initial ordering;
 @item @tt{udg} is as above, but only exploits unconditional parallel
                expressions @cite{annotators-jlp};
 @item @tt{urlp} exploits unconditional parallel expressions for NSIAP with
                @em{a posteriori} conditions @cite{nsicond-sas94}.
 @item @tt{crlp} exploits conditional parallel expressions for NSIAP with
                @em{a posteriori} conditions.

 @item @tt{granul} This transformation allows to perform run-time
 task granularity control of parallelized code 
 (see @cite{granularity-jsc-short}), so that the program will decide
 at run-time whether to run parallel expressions or not.
 The decision is based on the value of flag @tt{granularity\_threshold}.
@end{itemize}

@item for instrumenting the code for run-time assertion checking:
@begin{itemize}
 @item @tt{rtchecks} Transforms the program so that it will check the
       predicate-level assertions at run-time.
@end{itemize}
@end{itemize}
